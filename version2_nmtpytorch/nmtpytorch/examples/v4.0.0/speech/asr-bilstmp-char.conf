[train]
seed: 72000
model_type: ASR
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: cer,loss
eval_beam: 5
eval_batch_size: 16
save_best_metrics: True
eval_max_len: 400
n_checkpoints: 0
l2_reg: 0
gclip: 1
optimizer: adam
lr: 0.0004
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
batch_size: 36
save_path: /path/to/save/the/experiments
tensorboard_dir: ${save_path}/tb

[model]
att_type: mlp
att_bottleneck: hid
feat_dim: 43
enc_dim: 256
proj_dim: 256
emb_dim: 49
dec_dim: 256
dropout: 0.4
# 6 encoder layers
enc_layers: '1_1_2_2_1_1'
tied_dec_embs: True
dec_init: mean_ctx
bucket_by: en_speech
# Enough coverage @ 1500
max_len: 1500

direction: en_speech:Kaldi -> en_text:Text

[data]
root: /tmp/data/swbd

train_set: {'en_speech': '${root}/train_nodup',
            'en_text': '${root}/train_nodup/text.char.nmtpy'}

val_set: {'en_speech': '${root}/train_dev',
          'en_text': '${root}/train_dev/text.char.nmtpy'}

eval2000_set: {'en_speech': '${root}/eval2000_test'}

[vocabulary]
en_text: ${data:root}/train_nodup/text.char.vocab.nmtpy
